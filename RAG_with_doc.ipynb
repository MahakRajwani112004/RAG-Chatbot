{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99671c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDq9fS8HZrox70cm2bKYsQTdypqyPlWU6s\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def fetch_and_process_url(user_url):\n",
    "    \"\"\"Fetch content from a URL and store in ChromaDB.\"\"\"\n",
    "    url = f\"https://r.jina.ai/{user_url}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return \"Error fetching URL: \" + response.text\n",
    "    \n",
    "    markdown_content = response.text\n",
    "    summary_prompt = f\"\"\"Text:\n",
    "    {markdown_content}\n",
    "    Generate a detailed summary with headers (using # for each section).\"\"\"\n",
    "    response = model.generate_content(summary_prompt)\n",
    "    summary_text = response.text\n",
    "    \n",
    "    # Split and store in ChromaDB\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"Section\")])\n",
    "    documents = splitter.split_text(summary_text)\n",
    "    return store_in_chromadb(documents, \"URL content processed and stored.\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"Extracts text from a PDF and stores embeddings in ChromaDB.\"\"\"\n",
    "    if pdf_file is None:\n",
    "        return \"No file uploaded.\"\n",
    "    \n",
    "    loader = PyPDFLoader(pdf_file.name)\n",
    "    pages = loader.load()\n",
    "    full_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    return store_in_chromadb(documents, \"PDF content processed and stored.\")\n",
    "\n",
    "def store_in_chromadb(documents, message):\n",
    "    \"\"\"Stores extracted content in ChromaDB.\"\"\"\n",
    "    embedding_function = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "    vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"my_collection\", embedding_function=embedding_function)\n",
    "    vectorstore.delete_collection()\n",
    "    vectorstore = Chroma.from_documents(documents, embedding_function, persist_directory=\"./chroma_db\", collection_name=\"my_collection\")\n",
    "    return f\"âœ… {message}\"\n",
    "\n",
    "def chat_with_rag(query, chat_history_ui=None):\n",
    "    \"\"\"Handles user chat queries with retrieval from ChromaDB.\"\"\"\n",
    "    vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"my_collection\", embedding_function=OllamaEmbeddings(model=\"llama3.2:1b\"))\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    retrieved_content = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"No relevant content found.\"\n",
    "    print(\"Retrieved Context:\\n\", retrieved_content)  # Print retrieved context\n",
    "    \n",
    "    response_prompt = f\"\"\"\n",
    "    Use the following information to answer the question:\n",
    "    Context:\n",
    "    {retrieved_content}\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = model.generate_content(response_prompt)\n",
    "    bot_response = response.text\n",
    "    chat_history.append({\"role\": \"human\", \"human\": query})\n",
    "    chat_history.append({\"role\": \"ai\", \"ai\": bot_response})\n",
    "    \n",
    "    display_text = f\"Retrieved Content:\\n{retrieved_content}\\n\\nResponse:\\n{bot_response}\"\n",
    "    return chat_history_ui + [(query, display_text)] if chat_history_ui else [(query, display_text)]\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# ðŸ§  RAG Chatbot with URL & PDF Support\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        url_input = gr.Textbox(label=\"Enter URL\")\n",
    "        fetch_button = gr.Button(\"Fetch & Process URL\")\n",
    "    pdf_input = gr.File(label=\"Upload a PDF\")\n",
    "    process_pdf_button = gr.Button(\"Process PDF\")\n",
    "    output = gr.Textbox(label=\"Processing Status\")\n",
    "    \n",
    "    fetch_button.click(fetch_and_process_url, inputs=[url_input], outputs=[output])\n",
    "    process_pdf_button.click(extract_text_from_pdf, inputs=[pdf_input], outputs=[output])\n",
    "    \n",
    "    gr.Markdown(\"## ðŸ’¬ Chat Interface\")\n",
    "    chatbot_ui = gr.Chatbot()\n",
    "    chat_input = gr.Textbox(placeholder=\"Type your message...\")\n",
    "    chat_button = gr.Button(\"Send\")\n",
    "    chat_input.submit(chat_with_rag, inputs=[chat_input, chatbot_ui], outputs=[chatbot_ui])\n",
    "    chat_button.click(chat_with_rag, inputs=[chat_input, chatbot_ui], outputs=[chatbot_ui])\n",
    "\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
