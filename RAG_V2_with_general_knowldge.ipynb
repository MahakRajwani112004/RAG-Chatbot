{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdbae6a4-2267-4bca-805d-c45dbbb0a92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.41.2, however version 4.44.1 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDq9fS8HZrox70cm2bKYsQTdypqyPlWU6s\") # API Key\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "def fetch_and_process_url(user_url):\n",
    "    print(\"Step 1: Fetching content from URL...\")\n",
    "    url = f\"https://r.jina.ai/{user_url}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return \"Error fetching URL: \" + response.text\n",
    "\n",
    "    markdown_content = response.text\n",
    "    print(markdown_content)\n",
    "    print(\"Step 2: Generating summary...\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Text:\n",
    "{markdown_content}\n",
    "As a professional summarizer, create a concise and comprehensive summary of the provided text, while adhering to these guidelines:\n",
    "* Make section wise  detailed summary  with section header  and make sure all the information about a\n",
    "  particular section is included in it itself include (hash # sign infront of every header ).\n",
    "* Remove ** marks from the generated data ( keep everthing a plain text add your custom number or bullet points for better retrieval of results and for better \n",
    "  understanding of data to get answers.\n",
    "* Craft a summary that is detailed, thorough, in-depth, and complex, while maintaining clarity and conciseness.\n",
    "* Incorporate main ideas and essential information\n",
    "* Format the summary in paragraph and bullet points form for easy understanding.\n",
    "* Do not worry about length keep it more descriptive\n",
    "* There should be enough information on summary to get every answer from it in detail\n",
    "\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    summary_text = response.text\n",
    "    \n",
    "    print(\"Step 3: Summary generated.\")\n",
    "    print(summary_text)\n",
    "\n",
    "    # ðŸ”¹ Step 4: Splitting text into sections\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"Section\")])\n",
    "    documents = splitter.split_text(summary_text)\n",
    "\n",
    "    # ðŸ”¹ Step 5: Storing embeddings in ChromaDB\n",
    "    embedding_function = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "    vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"my_collection\", embedding_function=embedding_function)\n",
    "    vectorstore.delete_collection()\n",
    "    vectorstore = Chroma.from_documents(documents, embedding_function, persist_directory=\"./chroma_db\", collection_name=\"my_collection\")\n",
    "\n",
    "    print(\"âœ… Data successfully stored in ChromaDB!\")\n",
    "    return \"âœ… Data successfully stored in ChromaDB! You can now chat with the content.\"\n",
    "\n",
    "\n",
    "def rephrase_question(query):\n",
    "    print(\"ðŸ”„ Rephrasing follow-up question...\")\n",
    "    \n",
    "   \n",
    "    formatted_chat_history = [HumanMessage(content=msg[\"human\"]) if msg[\"role\"] == \"human\" else AIMessage(content=msg[\"ai\"]) for msg in chat_history]\n",
    "\n",
    "  \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Rephrase the user's follow-up question using the chat history.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ])\n",
    "     \n",
    "    rephrased_question = model.generate_content(prompt.format(chat_history=formatted_chat_history, query=query))\n",
    "    # for debugging \n",
    "    print(\"REPHRASED QUESTION \")\n",
    "    print(rephrased_question)\n",
    "    return rephrased_question.text.strip()\n",
    "    \n",
    "def chat_with_rag(query, chat_history_ui=None, use_general_knowledge=True): \n",
    "    print(\"Step 6: Retrieving relevant documents...\")\n",
    "    retrieved_docs = []\n",
    "    print(\"General Checkbox \" , use_general_knowledge)\n",
    "    # If General Knowledge is enabled, do not rephrase the query or use chat history\n",
    "    if use_general_knowledge:\n",
    "        reformulated_query = rephrase_question(query)\n",
    "\n",
    "        vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"my_collection\", embedding_function=OllamaEmbeddings(model=\"llama3.2:1b\"))\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        retrieved_docs = retriever.get_relevant_documents(reformulated_query)\n",
    "    \n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful chatbot with both document-based retrieval and general knowledge.\n",
    "\n",
    "        * Provide a comprehensive answer using both your own global knowledge base and the retrieved documents.\n",
    "        * Incorporate chat history and context to ensure a smooth and relevant response.\n",
    "        * Structure responses with a mix of descriptions and points, maintaining a natural flow.\n",
    "        * Avoid excessive sections; keep the response concise yet detailed.\n",
    "        * Ensure the response is as long as needed according to the question.\n",
    "        * Do not mention references to chat history, documents, or context in the response.\n",
    "        \n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        \n",
    "        Context from Retrieved Documents:\n",
    "        {context}\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # ðŸ”¹ Rephrase query based on chat history\n",
    "        reformulated_query = rephrase_question(query)\n",
    "\n",
    "        # ðŸ”¹ Retrieve relevant documents\n",
    "        vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"my_collection\", embedding_function=OllamaEmbeddings(model=\"llama3.2:1b\"))\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        retrieved_docs = retriever.get_relevant_documents(reformulated_query)\n",
    "    \n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are a chatbot that only relies on the retrieved documents.\n",
    "\n",
    "        * Do not use any external knowledge beyond the provided context.\n",
    "        * Do not mention references to chat history, documents, or context in the response.\n",
    "        * Ensure the response is detailed and well-structured with bullet points or paragraphs.\n",
    "\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "\n",
    "        Context from Retrieved Documents:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    bot_response = response.text if response and response.text else \"ðŸ¤– Sorry, I don't have information on that.\"\n",
    "\n",
    "    # ðŸ”¹ Only update chat history if using retrieval mode\n",
    "    if not use_general_knowledge:\n",
    "        chat_history.append({\"role\": \"human\", \"human\": query})\n",
    "        chat_history.append({\"role\": \"ai\", \"ai\": bot_response})\n",
    "\n",
    "    print(\"Step 10: Answer generated.\")\n",
    "    \n",
    "    return chat_history_ui + [(query, bot_response)] if chat_history_ui else [(query, bot_response)]\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# ðŸ§  RAG Chatbot with History-Aware Retrieval\")\n",
    "\n",
    "    with gr.Row():\n",
    "        url_input = gr.Textbox(label=\"Enter URL\")\n",
    "        fetch_button = gr.Button(\"Fetch & Process\")\n",
    "    \n",
    "    output = gr.Textbox(label=\"Processing Status\")\n",
    "    fetch_button.click(fetch_and_process_url, inputs=[url_input], outputs=[output])\n",
    "\n",
    "    gr.Markdown(\"## ðŸ’¬ Chat Interface\")\n",
    "\n",
    "    chatbot_ui = gr.Chatbot()\n",
    "\n",
    "    with gr.Row(): \n",
    "        chat_input = gr.Textbox(placeholder=\"Type your message...\", scale=4)\n",
    "        chat_button = gr.Button(\"Send\", scale=1)\n",
    "\n",
    "    # âœ… Checkbox to enable/disable general knowledge\n",
    "    use_general_knowledge_checkbox = gr.Checkbox(label=\"Use General Knowledge\", value=True)\n",
    "\n",
    "    # âœ… Pass checkbox value to chatbot function\n",
    "    chat_input.submit(chat_with_rag, \n",
    "                      inputs=[chat_input, chatbot_ui, use_general_knowledge_checkbox], \n",
    "                      outputs=[chatbot_ui])\n",
    "\n",
    "    chat_button.click(chat_with_rag, \n",
    "                      inputs=[chat_input, chatbot_ui, use_general_knowledge_checkbox], \n",
    "                      outputs=[chatbot_ui])\n",
    "\n",
    "app.launch()    \n",
    "    \n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
